# ğŸ§  NeuroNexus â€“ RAG Model (Ollama + Gemini + ChromaDB)

NeuroNexus is a **Retrieval-Augmented Generation (RAG)** based chatbot that can **read PDF documents and answer user queries contextually**.  
The system combines **local LLMs via Ollama** and **cloud-based Gemini API** with **ChromaDB vector storage** to deliver accurate, document-grounded responses.

This project is designed to run **locally on Windows** and focuses on **simplicity, reliability, and clean UI**, making it ideal for academic demos and project evaluations.

---

## ğŸš€ Features

- ğŸ“„ Upload and process PDF documents
- ğŸ” Context-aware question answering
- ğŸ§  Dual LLM support:
  - **Ollama (Local LLM fallback)**
  - **Google Gemini API (Primary cloud LLM)**
- ğŸ§© Automatic fallback if one model fails
- ğŸ“¦ Vector storage using **ChromaDB**
- ğŸ§  Token-based text chunking and embeddings
- ğŸ’¬ Chatbot-style interactive UI
- âš¡ Fast semantic search and response generation
- ğŸ–¥ï¸ Fully local execution (no production setup required)

---

## ğŸ—ï¸ Architecture (High-Level)

User Query
â†“
PDF Loader
â†“
Text Chunking
â†“
Embedding Generation
â†“
ChromaDB Vector Store
â†“
Relevant Context Retrieval
â†“
LLM (Gemini / Ollama)
â†“
Final Answer

---

## ğŸ› ï¸ Tech Stack

### ğŸ”¹ Backend & AI
- Python 3.10+
- LangChain
- Google Gemini API
- Ollama (Local LLMs)
- ChromaDB (Vector Database)

### ğŸ”¹ Embeddings
- Token-based text embeddings
- Semantic similarity search

### ğŸ”¹ Frontend / UI
- Streamlit (Chat-style interface)

---

## ğŸ“‚ Project Structure

NeuroNexus-RAG-Model/
â”‚
â”œâ”€â”€ main.py # Main application logic
â”œâ”€â”€ prompt.py # RAG prompt template
â”œâ”€â”€ env.py # API key loader
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ data/ # Uploaded PDF files
â”œâ”€â”€ chroma_db/ # Vector database storage
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .env # Environment variables (ignored)

---

## âš™ï¸ Installation & Setup (Windows)

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/xenonrocks71/NeuroNexus-RAG-Model.git
cd NeuroNexus-RAG-Model
2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ”‘ Environment Configuration

Create a .env file in the root directory:

GOOGLE_API_KEY=your_gemini_api_key_here


âš ï¸ Do NOT upload .env to GitHub

â–¶ï¸ Running the Application
streamlit run main.py


Then open your browser at:

http://localhost:8501

ğŸ§ª How It Works

User uploads a PDF

Text is extracted and split into chunks

Embeddings are generated for each chunk

Chunks are stored in ChromaDB

User query is embedded and matched

Relevant context is passed to:

Gemini API (primary)

Ollama (fallback)

Final grounded answer is generated

ğŸ¤– Supported Models
Ollama (Local)

LLaMA

Mistral

Gemma

Any Ollama-supported model

Gemini (Cloud)

Gemini Pro / Flash

ğŸ¯ Use Cases

Academic document analysis

Project reports Q&A

Research paper understanding

Internal knowledge assistants

AI demos and viva presentations

ğŸ“Œ Future Enhancements

Multi-PDF support

Conversation memory

Source citation in responses

Model selection toggle in UI

Docker support

ğŸ‘¨â€ğŸ’» Author

Mayur Patil
B.Tech Computer Engineering
Dr. Babasaheb Ambedkar Technological University, Lonere

â­ Acknowledgements

LangChain

Google Gemini

Ollama

ChromaDB

Streamlit

ğŸ“œ License

This project is for educational and academic purposes only.
